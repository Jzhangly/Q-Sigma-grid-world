{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044cc987",
   "metadata": {},
   "source": [
    "Environment based on https://towardsdatascience.com/implement-grid-world-with-q-learning-51151747b455<br>\n",
    "Very simple grid world with a wall at 1,1 <br>\n",
    "A win state at 1,3 <br>\n",
    "A lose state at 2,3 (Hole)<br>\n",
    "A start at 2,0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c3a53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (1, 3)\n",
    "LOSE_STATE = (2, 3)\n",
    "START = (2, 0)\n",
    "WALL = (1,1)\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[WIN_STATE] = 1\n",
    "        self.board[LOSE_STATE] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "\n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "\n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position on board\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            nxtState = (self.state[0] - 1, self.state[1])\n",
    "        elif action == 1:\n",
    "            nxtState = (self.state[0] + 1, self.state[1])\n",
    "        elif action == 2:\n",
    "            nxtState = (self.state[0], self.state[1] - 1)\n",
    "        else:\n",
    "            nxtState = (self.state[0], self.state[1] + 1)\n",
    "\n",
    "        # if next state is legal\n",
    "        if (nxtState[0] >= 0) and (nxtState[0] <= 2):\n",
    "            if (nxtState[1] >= 0) and (nxtState[1] <= 3):\n",
    "                if nxtState != (1, 1):\n",
    "                    return nxtState\n",
    "        return self.state\n",
    "\n",
    "    def showBoard(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if (i,j) == WIN_STATE:\n",
    "                    token = 'W'\n",
    "                elif (i,j) == LOSE_STATE:\n",
    "                    token = 'L'\n",
    "                elif (i,j) == START:\n",
    "                    token = 'S'\n",
    "                elif(i,j) == WALL:\n",
    "                    token = '#'\n",
    "                else:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e1856",
   "metadata": {},
   "source": [
    "Q(sigma) based on https://github.com/makaveli10/reinforcementLearning/blob/c06d3842e837e198642740eccd4f87d29dbde9b0/MultiStepBootstrapping/n_step_q_sigma.py#L21\n",
    "and Sutton and Barto 2020 Reinforcement learning book Avaliable from http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc6b4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, alpha = 0.2, gamma = 0.99, n = 3):\n",
    "        self.states = []  # record position and action taken at the position\n",
    "        self.actions = [0, 1, 2, 3] # 0 = up, 1 = down 2 = left 3 = right\n",
    "        self.State = State()\n",
    "        self.nA = 4\n",
    "        self.isEnd = self.State.isEnd\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actionsList = []\n",
    "        self.rewards = []\n",
    "        self.n = n\n",
    "\n",
    "        self.Q = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.Q[(i, j)] = {}\n",
    "                for a in self.actions:\n",
    "                    self.Q[(i, j)][a] = 0 \n",
    "        \n",
    "    def printHyper(self):\n",
    "        print(\"gamma = \", self.gamma)\n",
    "        print(\"alpha = \", self.alpha)\n",
    "        print(\"n-step =\", self.n)\n",
    "        \n",
    "    def bActionP(self,state,epsilon = 0.3):        \n",
    "        A = np.ones(self.nA, dtype=float) * (epsilon/self.nA)\n",
    "        temp = self.Q[state]\n",
    "        best_action = max(temp,key=temp.get)\n",
    "        A[best_action] += 1.0 - epsilon\n",
    "        return A\n",
    "    \n",
    "    def piActionP(self,state,epsilon = 0.01): #soft max\n",
    "        A = np.ones(self.nA, dtype=float) * (epsilon/self.nA)\n",
    "        temp = self.Q[state]\n",
    "        best_action = max(temp,key=temp.get)\n",
    "        A[best_action] += 1.0 - epsilon\n",
    "        \n",
    "        return A\n",
    "\n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(action)\n",
    "        # update State\n",
    "        return State(state=position)\n",
    "\n",
    "    def reset(self):\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "        \n",
    "    def printQ(self):\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                print(i,j)\n",
    "                print(self.Q[(i, j)])\n",
    "                    \n",
    "       \n",
    "    def chooseSigma(self):\n",
    "        return np.random.uniform(0, 1)\n",
    "        \n",
    "    def play(self, episodes=10):\n",
    "        for i in range(episodes):\n",
    "#             print(i)\n",
    "            self.reset()\n",
    "            T = np.inf\n",
    "            t = 0\n",
    "            tau = 0\n",
    "            n = self.n\n",
    "            stored_actions = {}\n",
    "            stored_states = {}\n",
    "            stored_rewards = {}\n",
    "            stored_pho = {}\n",
    "            stored_sigma={}\n",
    "            \n",
    "            self.reset()\n",
    "            \n",
    "            currentState = self.State.state\n",
    "            b_prob = self.bActionP(currentState)\n",
    "            pi_prob = self.piActionP(currentState)\n",
    "            action = np.random.choice(self.actions,p=b_prob)\n",
    "            pho = pi_prob[action] / b_prob[action]\n",
    "            \n",
    "            stored_actions[0] = action\n",
    "            stored_states[0] = currentState\n",
    "            stored_pho[0] = pho\n",
    "            stored_rewards[0] = 0\n",
    "            while True:\n",
    "#                 print(\"current position {} action {}\".format(self.State.state, action))\n",
    "                if t < T:\n",
    "                    self.State = self.takeAction(action)\n",
    "                    currentState = self.State.state\n",
    "                    reward = self.State.giveReward()\n",
    "                    stored_states[(t+1) % (n+1)] = self.State.state\n",
    "                    stored_rewards[(t+1) % (n+1)] = reward\n",
    "                    \n",
    "                    self.State.isEndFunc()\n",
    "#                    print(\"nxt state\", self.State.state)\n",
    " #                   print(\"---------------------\")\n",
    "                    self.isEnd = self.State.isEnd\n",
    "                    if self.State.isEnd:\n",
    "#                         print(reward)\n",
    "#                         print(\"WIN\")\n",
    "                        T = t + 1\n",
    "                    else:\n",
    "                        b_prob = self.bActionP(currentState)\n",
    "                        pi_prob = self.piActionP(currentState)\n",
    "                        action = np.random.choice(self.actions,p=b_prob)\n",
    "                        pho = pi_prob[action] / b_prob[action]\n",
    "                        stored_actions[(t+1)% (n+1)] = action\n",
    "                        sigma = self.chooseSigma()\n",
    "                        stored_pho[(t+1)% (n+1)] = pho\n",
    "                tau = t - self.n + 1\n",
    "                if tau >= 0:\n",
    "                    if t + 1 < T:\n",
    "                        G = self.Q[stored_states[(t+1)% (n+1)]][stored_actions[(t+1)% (n+1)]]\n",
    "                    for k in range(min(t+1, T), tau, -1):\n",
    "                        if k == T:\n",
    "\n",
    "                            G = stored_rewards[T% (n+1)]\n",
    "                        else:\n",
    "                            s_k = stored_states[k% (n+1)]\n",
    "                            a_k = stored_actions[k% (n+1)]\n",
    "                            r_k = stored_rewards[k% (n+1)]\n",
    "                            pho_k = stored_pho[k% (n+1)]\n",
    "                            \n",
    "                            VBar = np.sum([(self.piActionP(s_k)[a]) * self.Q[s_k][a] for a in range(self.nA)])\n",
    "\n",
    "                            G = r_k + self.gamma * ((sigma * pho_k) + ((1-sigma) * self.piActionP(s_k)[a_k])) * (G - self.Q[s_k][a_k])+ self.gamma * VBar\n",
    "                    s_tau = stored_states[tau% (n+1)]\n",
    "                    a_tau = stored_actions[tau% (n+1)]\n",
    "\n",
    "                    self.Q[s_tau][a_tau] = self.Q[s_tau][a_tau] + self.alpha * (G - self.Q[s_tau][a_tau])\n",
    "                if tau >= (T-1):\n",
    "                    break\n",
    "                else:\n",
    "                    t = t + 1\n",
    "                            \n",
    "    def replay(self):\n",
    "        self.reset()\n",
    "        stored_states = []\n",
    "        stored_actions = {}\n",
    "        isEnd = False\n",
    "        state = self.State.state\n",
    "        stored_states.append(state)\n",
    "        while (isEnd == False):\n",
    "            \n",
    "            temp = self.Q[state]\n",
    "            action = max(temp,key=temp.get)\n",
    "            if action == 0:\n",
    "                stored_actions[state] = '^'\n",
    "            elif action == 1:\n",
    "                stored_actions[state] = 'v'\n",
    "            elif action == 3:\n",
    "                stored_actions[state] = '>'\n",
    "            elif action == 4:\n",
    "                stored_actions[state] = '<'\n",
    "            self.State = self.takeAction(action)\n",
    "            state = self.State.state\n",
    "            stored_states.append(state)\n",
    "            self.State.isEndFunc() \n",
    "            isEnd = self.State.isEnd\n",
    "            \n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if (i,j) == WIN_STATE:\n",
    "                    token = 'W'\n",
    "                elif (i,j) == LOSE_STATE:\n",
    "                    token = 'L'\n",
    "                elif (i,j) == START:\n",
    "                    token = 'S'\n",
    "                elif(i,j) == WALL:\n",
    "                    token = '#'\n",
    "                elif((i,j) in stored_states):\n",
    "                    token = stored_actions[(i,j)]\n",
    "                else:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3905ea2",
   "metadata": {},
   "source": [
    "Chosen hyperparameters:<br>\n",
    "\n",
    "Learning rate(alpha) = 0.2<br>\n",
    "Decay(gamma) = 0.99<br>\n",
    "N-step = 3<br>\n",
    "\n",
    "Actionspace<br>\n",
    "0 = up<br>\n",
    "1 = down<br>\n",
    "2 = left<br>\n",
    "3 = right<br>\n",
    "<br>\n",
    "Play 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "954fd6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| 0 | 0 | 0 | 0 | \n",
      "-----------------\n",
      "| 0 | # | 0 | W | \n",
      "-----------------\n",
      "| S | 0 | 0 | L | \n",
      "-----------------\n",
      "Hyperparameters \n",
      "\n",
      "gamma =  0.99\n",
      "alpha =  0.2\n",
      "n-step = 3\n",
      "Q-values ... \n",
      "\n",
      "0 0\n",
      "{0: 0.8899384404250286, 1: 0.841080164938459, 2: 0.8516756853196443, 3: 0.9669093638975295}\n",
      "0 1\n",
      "{0: 0.835611487833921, 1: 0.8614485567464135, 2: 0.7944791943040375, 3: 0.9778812709589971}\n",
      "0 2\n",
      "{0: 0.7749393935026497, 1: 0.8535126183289126, 2: 0.8802462448093975, 3: 0.9883177207031081}\n",
      "0 3\n",
      "{0: 0.6648992478675471, 1: 0.999999999681713, 2: 0.6554274132861289, 3: 0.8762367652672477}\n",
      "1 0\n",
      "{0: 0.9569780878273034, 1: 0.8653590296693154, 2: 0.7960015420828969, 3: 0.7753943016979519}\n",
      "1 1\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "1 2\n",
      "{0: 0.9530757539396533, 1: 0.3668605048065946, 2: 0.13413738055317637, 3: 0.2}\n",
      "1 3\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "2 0\n",
      "{0: 0.9467481681248493, 1: 0.6807487360610697, 2: 0.8519748781497763, 3: 0.6518738169599784}\n",
      "2 1\n",
      "{0: 0.00018411072057645698, 1: 0, 2: 0.18082970912558424, 3: 0.7604544336732746}\n",
      "2 2\n",
      "{0: 0.8254052163724693, 1: 0, 2: 0.13553739901028813, 3: -0.2}\n",
      "2 3\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "\n",
      "\n",
      "Optimal play via max q values\n",
      "\n",
      "-----------------\n",
      "| > | > | > | v | \n",
      "-----------------\n",
      "| ^ | # | 0 | W | \n",
      "-----------------\n",
      "| S | 0 | 0 | L | \n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ag = Agent(alpha = 0.2, gamma = 0.99, n = 3)\n",
    "st = State()\n",
    "st.showBoard()\n",
    "print(\"Hyperparameters \\n\")\n",
    "ag.printHyper()\n",
    "ag.play(100)\n",
    "    \n",
    "print(\"Q-values ... \\n\")\n",
    "ag.printQ()\n",
    "print(\"\\n\")\n",
    "print(\"Optimal play via max q values\\n\")\n",
    "ag.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb9a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
